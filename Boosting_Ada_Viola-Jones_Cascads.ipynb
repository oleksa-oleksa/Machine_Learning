{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the spam dataset:\n",
    "\n",
    "data = np.array(pd.read_csv('../data/spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "#  zeros labels must be negative (-1) for AdaBoost\n",
    "y[y == 0] = -1 \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 1. AdaBoot \n",
    "Implement AdaBoost using Python (incl. Numpy etc.) and use it on the SPAM-Dataset\n",
    "\n",
    "1.The weak classifiers should be decision stumps (i.e. decision trees with one node).\n",
    "\n",
    "(a) Print a confusion matrix.\n",
    "\n",
    "(b) Is AdaBoost better when using stronger weak learners? Why or why not? Compare your results to using depth-2 decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A decision stump is a machine learning model consisting of a one-level decision tree.\n",
    "# That is, it is a decision tree with one internal node (the root) \n",
    "# which is immediately connected to the terminal nodes (its leaves).\n",
    "\n",
    "# For continuous features, usually, some threshold feature value is selected, \n",
    "# and the stump contains two leaves â€” for values below and above the threshold. \n",
    "class OneLevelDecisionTree():\n",
    "    def fit(self, feature_column, neg_label, pos_label, feature):\n",
    "        self.feature = feature_column\n",
    "        self.neg_label = neg_label\n",
    "        self.pos_label = pos_label\n",
    "        self.threshold = feature\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        \n",
    "        if self.feature >= self.threshold:\n",
    "            return self.neg_label\n",
    "        else:\n",
    "            return self.pos_label\n",
    "            \n",
    "    \n",
    "def classifier_boost(X, cls):\n",
    "    boost = []\n",
    "    neg_label = -1\n",
    "    pos_label = 1\n",
    "    \n",
    "    for column in range(X.shape[1]):\n",
    "        # passing by columns\n",
    "        # get every unique feature in ordered way\n",
    "        feature_set = sorted(set(X[:, column]))\n",
    "        \n",
    "        # threshold feature value is selected for every feature\n",
    "        for threshold in feature_set:\n",
    "            boost.append(cls.fit(column, neg_label, pos_label, threshold))\n",
    "            \n",
    "    return boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, cls_boost, boost_size):\n",
    "        self.cls_boost = cls_boost\n",
    "        self.boost_size = boost_size       \n",
    "        self.classifiers = []\n",
    "        self.weights = []\n",
    "        self.error = []\n",
    "        \n",
    "    # from the lecture:\n",
    "    # error calculation looks how many times the prediction\n",
    "    # of the model was wrong\n",
    "    def compute_error(self, preds):\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i] != self.y[i]:\n",
    "                self.error.append(self.weights[i])\n",
    "                \n",
    "        \n",
    "    def fit_train(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Step 1\n",
    "        # initialise w_i = 1/N for all i from [1..N]\n",
    "        N = len(y) + 1\n",
    "        for i in range(1, N):\n",
    "            w = 1/N\n",
    "            self.weights.append(w)\n",
    "        \n",
    "        # Step 2.a - 1\n",
    "        # for m from [1..M] of boost size\n",
    "        # train a classifier f_m(x) -> [-1, 1] on X\n",
    "        # --> DONE with classifier_boost() and saved as self.cls_boost\n",
    "        \n",
    "        # Step 2.a - 2\n",
    "        # here is probably supposed to be a prediction step...\n",
    "        for cls in self.cls_boost:\n",
    "            predictions.append(cls.predict(X))\n",
    "        \n",
    "        # Step 2.b\n",
    "        # Compute classification error    \n",
    "        for m in range(1, self.boost_size + 1):\n",
    "            self.compute_error(predictions)\n",
    "            # from lecture: sum of all the weights that were missclassified \n",
    "            # devided by the sum of all weights\n",
    "            E_m = np.sum(self.error) / np.sum(self.weights)\n",
    "            \n",
    "            # Step 2.c\n",
    "            # Compute classifier weight\n",
    "            \n",
    "            alpha_m = np.log((1 - E_m) / E_m)\n",
    "            \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        scouting_matrix = np.array([clf.predict(X) != y for clf in self.classifier_pool])\n",
    "        w = np.ones(len(y)) # Initially, all weights are the same\n",
    "        \n",
    "        for _ in range(self.num_classifiers):\n",
    "            # Step 1\n",
    "            errors = scouting_matrix@w\n",
    "            best_remaining = errors.argmin()\n",
    "            \n",
    "            # Step 2\n",
    "            We = errors[best_remaining]\n",
    "            W = w.sum()\n",
    "            em = (W - We) / W\n",
    "                        \n",
    "            self.classifiers += [self.classifier_pool[best_remaining]]\n",
    "            self.weights += [0.5 * -np.log((1 - em) / em)] # alphas\n",
    "            \n",
    "            # Step 3\n",
    "            w = w * np.exp(np.where(scouting_matrix[best_remaining], 1, -1) * self.weights[-1])\n",
    "            scouting_matrix = np.delete(scouting_matrix, best_remaining, axis=0)\n",
    "            del self.classifier_pool[best_remaining]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.array([cl.predict(X) for cl in self.classifiers])\n",
    "        weighted_preds = np.dot(self.weights, preds)\n",
    "        return np.where(weighted_preds >= 0, 1, -1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak: 13293 of type OneLevelDecisionTree\n"
     ]
    }
   ],
   "source": [
    "one_cls = OneLevelDecisionTree()\n",
    "boost = classifier_boost(X_train, one_cls)\n",
    "print (\"Weak: {} of type {}\".format(len(boost), type(one_cls).__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_size = 50\n",
    "ada = AdaBoost(boost, boost_size)\n",
    "ada.fit_train(X_train, y_train)\n",
    "ada_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(labels, predictions):\n",
    "        return np.mean(labels == predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ada_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4ab3647b221a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mada_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ada_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(y_train, ada_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So I will use my code from previous assignment to build a one\n",
    "# THIS CELL IS COPIED FROM THE PREVIOUS ASSIGNMENT AND CONTAINS IMPLEMENTATION FOR A DECISSION TREE \n",
    "\n",
    "\n",
    "def cross_entropy(p):\n",
    "        if p == 1 or p == 0: \n",
    "            # The entropy is zero if one event is certain\n",
    "            return 0\n",
    "        return - (p * np.log(p) + (1-p) * np.log((1-p)))\n",
    "\n",
    "# Weight of a child node is number of samples in the node/total samples of all child nodes. \n",
    "# Similarly information gain is calculated with gini score. \n",
    "def children_entropy(feature, y):\n",
    "    right = (feature == True).sum()/len(feature)\n",
    "    left = 1 - right\n",
    "    \n",
    "    p = np.sum(y[feature])/len(y[feature]) \n",
    "    q = np.sum(y[np.invert(feature)])/len(y[np.invert(feature)])\n",
    "    \n",
    "    entropy_right = right * cross_entropy(p)\n",
    "    entropy_left = left * cross_entropy(q)\n",
    "    total_entropy = entropy_right + entropy_left\n",
    "    return total_entropy, q, p\n",
    "\n",
    "#====================================\n",
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self, height=7):\n",
    "        self.min_size = 4\n",
    "        self.height = height\n",
    "    \n",
    "    # fit a basic binary tree for 2 classes classificaton \n",
    "    def fit(self, X, y):\n",
    "        self.tree_size = 2**self.height - 1\n",
    "        #print(self.tree_size)\n",
    "        self.tmp_size = 2**(self.height + 1) - 1\n",
    "        self.features = X.shape[1]\n",
    "        self.tree = np.full(self.tmp_size, -1)\n",
    "        self.tree_tmp = np.full(self.tmp_size + 1, -1)\n",
    "        self.split_tree(X, y, 0)\n",
    "    \n",
    "    # binary tree\n",
    "    def left_tree(self, leaf):\n",
    "        return 2 * leaf + 1\n",
    "    \n",
    "    def right_tree(self, leaf):\n",
    "        return 2 * leaf + 2\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            idx = 0\n",
    "            leaf = self.tree[idx]\n",
    "            while self.tree[self.left_tree(idx)] != -1 or self.tree[self.right_tree(idx)] != -1:\n",
    "                #print(\"idx:\", idx)\n",
    "                #print(\"leaf:\", idx)\n",
    "\n",
    "                if leaf >= self.tree_size:\n",
    "                    return\n",
    "                \n",
    "                if x[leaf]:\n",
    "                    idx = self.right_tree(idx)\n",
    "                    #print(\"--------> right\")\n",
    "                else:\n",
    "                    idx = self.left_tree(idx)\n",
    "                    #print(\"left <--------\")\n",
    "                prediction = self.tree_tmp[idx]\n",
    "                leaf = self.tree[idx]\n",
    "            predictions += [prediction]\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def split_data(self, index, value, X):\n",
    "        left, right = list(), list()\n",
    "        for row in X:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "        \n",
    "    \n",
    "    def split_tree(self, X, y, leaf):\n",
    "  \n",
    "        # parent node is a leaf\n",
    "        #print(\"leaf\", leaf)\n",
    "        if leaf >= self.tree_size:\n",
    "            return\n",
    "        \n",
    "        entropies = np.full(self.features, np.inf) \n",
    "        left = np.empty(self.features)\n",
    "        right = np.empty(self.features)\n",
    "        \n",
    "        # for every feature variable\n",
    "        for i, feature in enumerate(X.T):\n",
    "            if np.sum(feature) == 0 or np.sum(np.invert(feature)) == 0:\n",
    "                continue \n",
    "            entropies[i], left[i], right[i] = children_entropy(feature, y)\n",
    "        \n",
    "        min_entropy = np.argmin(entropies)\n",
    "        \n",
    "        right = X[:,min_entropy]\n",
    "        left = np.invert(right)\n",
    "        #print(left)\n",
    "        \n",
    "        #print(\"min_entropy\", min_entropy)\n",
    "        self.tree[leaf] = min_entropy\n",
    "        if min_entropy < len(self.tree_tmp):\n",
    "            if (min_entropy < len(left)) and (min_entropy < len(right)):\n",
    "                self.tree_tmp[self.left_tree(leaf)] = left[min_entropy]\n",
    "                self.tree_tmp[self.right_tree(leaf)] = right[min_entropy]\n",
    "        \n",
    "        if len(y[right]) == 0 or len(y[left]) == 0:\n",
    "            return\n",
    "        # grow tree \n",
    "        if leaf >= self.min_size:\n",
    "            return\n",
    "        self.split_tree(X[left], y[left], self.left_tree(leaf))\n",
    "        self.split_tree(X[right], y[right], self.right_tree(leaf))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 2 (Bonus).\n",
    "\n",
    "Viola-Jones Face Detection\n",
    "Implement the Viola-Jones algorithm (without the cascade mechanism) and use it on a LFW-Face-subsetto classify faces.\n",
    "\n",
    "(a) Visualize the top ten face classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 3 (Bonus).\n",
    "Cascade-Classification\n",
    "\n",
    "Implement a cascade algorithm to classify faces in a picture of your choice \n",
    "(there should be more than a face on your image, e.g. skimage.data.astronaut())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

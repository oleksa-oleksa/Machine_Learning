{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the spam dataset:\n",
    "\n",
    "data = np.array(pd.read_csv('../data/spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "#  zeros labels must be negative (-1) for AdaBoost\n",
    "y[y == 0] = -1 \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 1. AdaBoot \n",
    "Implement AdaBoost using Python (incl. Numpy etc.) and use it on the SPAM-Dataset\n",
    "\n",
    "1.The weak classifiers should be decision stumps (i.e. decision trees with one node).\n",
    "\n",
    "(a) Print a confusion matrix.\n",
    "\n",
    "(b) Is AdaBoost better when using stronger weak learners? Why or why not? Compare your results to using depth-2 decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A decision stump is a machine learning model consisting of a one-level decision tree.\n",
    "# That is, it is a decision tree with one internal node (the root) \n",
    "# which is immediately connected to the terminal nodes (its leaves).\n",
    "\n",
    "# For continuous features, usually, some threshold feature value is selected, \n",
    "# and the stump contains two leaves â€” for values below and above the threshold. \n",
    "class OneLevelDecisionTree():\n",
    "    def fit(self, feature_column, neg_label, pos_label, feature):\n",
    "        self.feature = feature_column\n",
    "        self.neg_label = neg_label\n",
    "        self.pos_label = pos_label\n",
    "        self.threshold = feature\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        \n",
    "        return np.where(X[:, self.feature] >= self.threshold, self.neg_label, self.pos_label)\n",
    "    \n",
    "def classifier_boost(X, cls):\n",
    "    boost = []\n",
    "    neg_label = -1\n",
    "    pos_label = 1\n",
    "    \n",
    "    for column in range(X.shape[1]):\n",
    "        # passing by columns\n",
    "        # get every unique feature in ordered way\n",
    "        feature_set = sorted(set(X[:, column]))\n",
    "        \n",
    "        # threshold feature value is selected for every feature\n",
    "        for threshold in feature_set:\n",
    "            boost += [\n",
    "                cls.fit(column, neg_label, pos_label, threshold),\n",
    "                cls.fit(column, pos_label, neg_label, threshold)]\n",
    "    return boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class AdaBoost():\n",
    "    def __init__(self, cls_boost, boost_size):\n",
    "        self.cls_boost = cls_boost\n",
    "        self.boost_size = boost_size\n",
    "        self.classifiers = []\n",
    "        self.weights = []\n",
    "        self.error = []\n",
    "        \n",
    "    # from the lecture:\n",
    "    # error calculation looks how many times the prediction\n",
    "    # of the model was wrong\n",
    "    # ---> finaly replaced with inside the fit_train with a numpy boolean expression\n",
    "    def compute_error(self, preds):\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i] != self.y[i]:\n",
    "                self.error.append(self.weights[i])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        \n",
    "    def fit_train(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "                \n",
    "        # Step 1\n",
    "        # initialise w_i = 1/N for all i from [1..N]\n",
    "        # w = np.full_like(y, 1/len(y))\n",
    "        \n",
    "        # with the 1/N the each weight is too small = 0.00028986\n",
    "        # trying with 1 as a initial weight\n",
    "        w = np.ones(len(y))\n",
    "\n",
    "        # Step 2.a - 1\n",
    "        # for m from [1..M] of boost size\n",
    "        # train a classifier f_m(x) -> [-1, 1] on X\n",
    "        # and save all errors\n",
    "        self.error = np.array([clf.predict(X) != y for clf in self.cls_boost])\n",
    " \n",
    "        for m in range(self.boost_size):                   \n",
    "            # Step 2.b\n",
    "            # Compute classification error    \n",
    "            errors = np.matmul(self.error, w)\n",
    "\n",
    "            # indix of the minimum values\n",
    "            min_err_idx = np.argmin(errors)\n",
    "            self.classifiers += [self.cls_boost[min_err_idx]]\n",
    "            \n",
    "            # from lecture: sum of all the weights that were missclassified \n",
    "            # devided by the sum of all weights\n",
    "            w_err = self.error[min_err_idx]\n",
    "            w_sum = w.sum()\n",
    "\n",
    "            E_m = (w_sum - w_err) / w_sum\n",
    "\n",
    "            # Step 2.c\n",
    "            # Compute classifier weight\n",
    "\n",
    "            alpha_m = 0.5 * np.log((1 - E_m) / E_m)\n",
    "\n",
    "\n",
    "            # Step 2.d\n",
    "            # Recompute sample weights\n",
    "            self.weights.append(alpha_m)\n",
    "            \n",
    "            w = w * np.exp(np.where(w_err, 1, -1) * self.weights[-1])\n",
    "            \n",
    "            self.error = np.delete(self.error, min_err_idx, axis=0)\n",
    "            del self.cls_boost[min_err_idx]\n",
    "            \n",
    "            print(\"debug: done m = \", m)\n",
    "            print(\"min_err_idx: \", min_err_idx)\n",
    "            print(\"w_sum: \", w_sum)\n",
    "            print(\"w_err: \", w_err)\n",
    "            print(\"E_m: \", E_m)\n",
    "            print(\"alpha_m : \", alpha_m )\n",
    "\n",
    "\n",
    "        # convert into numpy array\n",
    "        self.weights = np.array(self.weights)\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        # print(self.weights.shape) -> (50, 3450)\n",
    "        predictions = []\n",
    "        weighted_preds = []                                                     \n",
    "                                                             \n",
    "        for cls in self.classifiers:\n",
    "            predictions.append(cls.predict(X))\n",
    "            \n",
    "        predictions = np.array(predictions)\n",
    "        # print(predictions.shape) --> (50, 1151)\n",
    "                                                             \n",
    "        weighted_preds = np.matmul(predictions.T, self.weights)\n",
    "        \n",
    "        print(weighted_preds)\n",
    "        # return with sign\n",
    "        return np.where(weighted_preds > 0, 1, -1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak: 26586 of type OneLevelDecisionTree\n"
     ]
    }
   ],
   "source": [
    "one_cls = OneLevelDecisionTree()\n",
    "boost = classifier_boost(X_train, one_cls)\n",
    "print (\"Weak: {} of type {}\".format(len(boost), type(one_cls).__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:60: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: done m =  0\n",
      "min_err_idx:  0\n",
      "w_sum:  3450.0\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [0.99971014 1.         1.         ... 0.99971014 1.         1.        ]\n",
      "alpha_m :  [-4.07291981        -inf        -inf ... -4.07291981        -inf\n",
      "        -inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in matmul\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: done m =  1\n",
      "min_err_idx:  0\n",
      "w_sum:  inf\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  2\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  3\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  4\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  5\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  6\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  7\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  8\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  9\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  10\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  11\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  12\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  13\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  14\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  15\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  16\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  17\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  18\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "debug: done m =  19\n",
      "min_err_idx:  0\n",
      "w_sum:  nan\n",
      "w_err:  [ True False False ...  True False False]\n",
      "E_m:  [nan nan nan ... nan nan nan]\n",
      "alpha_m :  [nan nan nan ... nan nan nan]\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:99: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "boost_size = 20\n",
    "ada = AdaBoost(boost, boost_size)\n",
    "ada.fit_train(X_train, y_train)\n",
    "ada_predictions = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(labels, predictions):\n",
    "        return np.mean(labels == predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6060869565217392\n"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(y_train, ada_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So I will use my code from previous assignment to build a one\n",
    "# THIS CELL IS COPIED FROM THE PREVIOUS ASSIGNMENT AND CONTAINS IMPLEMENTATION FOR A DECISSION TREE \n",
    "\n",
    "\n",
    "def cross_entropy(p):\n",
    "        if p == 1 or p == 0: \n",
    "            # The entropy is zero if one event is certain\n",
    "            return 0\n",
    "        return - (p * np.log(p) + (1-p) * np.log((1-p)))\n",
    "\n",
    "# Weight of a child node is number of samples in the node/total samples of all child nodes. \n",
    "# Similarly information gain is calculated with gini score. \n",
    "def children_entropy(feature, y):\n",
    "    right = (feature == True).sum()/len(feature)\n",
    "    left = 1 - right\n",
    "    \n",
    "    p = np.sum(y[feature])/len(y[feature]) \n",
    "    q = np.sum(y[np.invert(feature)])/len(y[np.invert(feature)])\n",
    "    \n",
    "    entropy_right = right * cross_entropy(p)\n",
    "    entropy_left = left * cross_entropy(q)\n",
    "    total_entropy = entropy_right + entropy_left\n",
    "    return total_entropy, q, p\n",
    "\n",
    "#====================================\n",
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self, height=7):\n",
    "        self.min_size = 4\n",
    "        self.height = height\n",
    "    \n",
    "    # fit a basic binary tree for 2 classes classificaton \n",
    "    def fit(self, X, y):\n",
    "        self.tree_size = 2**self.height - 1\n",
    "        #print(self.tree_size)\n",
    "        self.tmp_size = 2**(self.height + 1) - 1\n",
    "        self.features = X.shape[1]\n",
    "        self.tree = np.full(self.tmp_size, -1)\n",
    "        self.tree_tmp = np.full(self.tmp_size + 1, -1)\n",
    "        self.split_tree(X, y, 0)\n",
    "    \n",
    "    # binary tree\n",
    "    def left_tree(self, leaf):\n",
    "        return 2 * leaf + 1\n",
    "    \n",
    "    def right_tree(self, leaf):\n",
    "        return 2 * leaf + 2\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            idx = 0\n",
    "            leaf = self.tree[idx]\n",
    "            while self.tree[self.left_tree(idx)] != -1 or self.tree[self.right_tree(idx)] != -1:\n",
    "                #print(\"idx:\", idx)\n",
    "                #print(\"leaf:\", idx)\n",
    "\n",
    "                if leaf >= self.tree_size:\n",
    "                    return\n",
    "                \n",
    "                if x[leaf]:\n",
    "                    idx = self.right_tree(idx)\n",
    "                    #print(\"--------> right\")\n",
    "                else:\n",
    "                    idx = self.left_tree(idx)\n",
    "                    #print(\"left <--------\")\n",
    "                prediction = self.tree_tmp[idx]\n",
    "                leaf = self.tree[idx]\n",
    "            predictions += [prediction]\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def split_data(self, index, value, X):\n",
    "        left, right = list(), list()\n",
    "        for row in X:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "        \n",
    "    \n",
    "    def split_tree(self, X, y, leaf):\n",
    "  \n",
    "        # parent node is a leaf\n",
    "        #print(\"leaf\", leaf)\n",
    "        if leaf >= self.tree_size:\n",
    "            return\n",
    "        \n",
    "        entropies = np.full(self.features, np.inf) \n",
    "        left = np.empty(self.features)\n",
    "        right = np.empty(self.features)\n",
    "        \n",
    "        # for every feature variable\n",
    "        for i, feature in enumerate(X.T):\n",
    "            if np.sum(feature) == 0 or np.sum(np.invert(feature)) == 0:\n",
    "                continue \n",
    "            entropies[i], left[i], right[i] = children_entropy(feature, y)\n",
    "        \n",
    "        min_entropy = np.argmin(entropies)\n",
    "        \n",
    "        right = X[:,min_entropy]\n",
    "        left = np.invert(right)\n",
    "        #print(left)\n",
    "        \n",
    "        #print(\"min_entropy\", min_entropy)\n",
    "        self.tree[leaf] = min_entropy\n",
    "        if min_entropy < len(self.tree_tmp):\n",
    "            if (min_entropy < len(left)) and (min_entropy < len(right)):\n",
    "                self.tree_tmp[self.left_tree(leaf)] = left[min_entropy]\n",
    "                self.tree_tmp[self.right_tree(leaf)] = right[min_entropy]\n",
    "        \n",
    "        if len(y[right]) == 0 or len(y[left]) == 0:\n",
    "            return\n",
    "        # grow tree \n",
    "        if leaf >= self.min_size:\n",
    "            return\n",
    "        self.split_tree(X[left], y[left], self.left_tree(leaf))\n",
    "        self.split_tree(X[right], y[right], self.right_tree(leaf))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 2 (Bonus).\n",
    "\n",
    "Viola-Jones Face Detection\n",
    "Implement the Viola-Jones algorithm (without the cascade mechanism) and use it on a LFW-Face-subsetto classify faces.\n",
    "\n",
    "(a) Visualize the top ten face classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolaJonesFaceDetection():\n",
    "    def __init__(self, img):\n",
    "        self.width = img.shape[1]\n",
    "        self.height = img.shape[0]\n",
    "        self.img = img\n",
    "        # integral image to be calculated\n",
    "        self.integral_img = np.zeros_like(img)\n",
    "    \n",
    "    # https://en.wikipedia.org/wiki/Summed-area_table\n",
    "    # The summed-area table can be computed efficiently in a single pass over the image, \n",
    "    # as the value in the summed-area table at (x, y) is:\n",
    "    # I(x,y)= i(x,y) +I(x,y-1) +I(x-1,y)- I(x-1,y-1)\n",
    "    \n",
    "    def calc_integral_image(self):\n",
    "        for y in self.height:\n",
    "            for x in self.width:\n",
    "                self.integral_img[x, y] = self.img[x, y] + self.integral_img[x, y - 1] \n",
    "                + self.integral_img[x - 1, y] - self.integral_img[x - 1, y - 1]\n",
    "        \n",
    "        return self.integral_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 3 (Bonus).\n",
    "Cascade-Classification\n",
    "\n",
    "Implement a cascade algorithm to classify faces in a picture of your choice \n",
    "(there should be more than a face on your image, e.g. skimage.data.astronaut())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

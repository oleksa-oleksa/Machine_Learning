{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the spam dataset:\n",
    "\n",
    "data = np.array(pd.read_csv('../data/spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "#  zeros labels must be negative (-1) for AdaBoost\n",
    "y[y == 0] = -1 \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 1. AdaBoot \n",
    "Implement AdaBoost using Python (incl. Numpy etc.) and use it on the SPAM-Dataset\n",
    "\n",
    "1.The weak classifiers should be decision stumps (i.e. decision trees with one node).\n",
    "\n",
    "(a) Print a confusion matrix.\n",
    "\n",
    "(b) Is AdaBoost better when using stronger weak learners? Why or why not? Compare your results to using depth-2 decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A decision stump is a machine learning model consisting of a one-level decision tree.\n",
    "# That is, it is a decision tree with one internal node (the root) \n",
    "# which is immediately connected to the terminal nodes (its leaves).\n",
    "\n",
    "# For continuous features, usually, some threshold feature value is selected, \n",
    "# and the stump contains two leaves â€” for values below and above the threshold. \n",
    "class OneLevelDecisionTree():\n",
    "    def __init__(self, feature_column, label_1, label_2, threshold=0):\n",
    "        self.feature = feature_column\n",
    "        self.label_1 = label_1\n",
    "        self.label_2 = label_2\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.feature] >= self.threshold, self.label_1, self.label_2)\n",
    "    \n",
    "def classifier_boost(X):\n",
    "    boost = []   \n",
    "    # passing by columns\n",
    "    # creates the same number of stumps as the number of features. \n",
    "    for feature in range(X.shape[1]):\n",
    "        # get every unique feature in ordered way\n",
    "        for threshold in sorted(set(X[:, feature])):\n",
    "            boost += [\n",
    "                OneLevelDecisionTree(feature, 1, -1, threshold)]  \n",
    "            '''\n",
    "            boost += [\n",
    "                OneLevelDecisionTree(feature, 1, -1, threshold),\n",
    "                OneLevelDecisionTree(feature, -1, 1, threshold)\n",
    "            ] \n",
    "            '''\n",
    "    return boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class AdaBoost():\n",
    "    def __init__(self, cls_boost, boost_size):\n",
    "        self.cls_boost = cls_boost\n",
    "        self.boost_size = boost_size\n",
    "        self.classifiers = []\n",
    "        self.weights = []\n",
    "        \n",
    "    # from the lecture:\n",
    "    # error calculation looks how many times the prediction\n",
    "    # of the model was wrong\n",
    "    # ---> finaly replaced with inside the fit_train with a numpy boolean expression\n",
    "    def compute_error(self, preds):\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i] != self.y[i]:\n",
    "                self.error.append(self.weights[i])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        \n",
    "    def fit_train(self, X, y): \n",
    "        # Step 1\n",
    "        # initialise w_i = 1/N for all i from [1..N]\n",
    "        w = np.full_like(y, 1/len(y))\n",
    "        \n",
    "        # with the 1/N the each weight is too small = 0.00028986\n",
    "        # trying with 1 as a initial weight\n",
    "        # w = np.ones(len(y))\n",
    "        \n",
    "        # P.S: it does not change the predictions\n",
    "        # the np.log in alpha should be with minus sign\n",
    "\n",
    "        # Step 2.a - 1\n",
    "        # for m from [1..M] of boost size\n",
    "        # train a classifier f_m(x) -> [-1, 1] on X\n",
    "        # and save all errors\n",
    "        # diffs.shape:  (13292, 3450)\n",
    "        diffs = np.array([clf.predict(X) != y for clf in self.cls_boost])\n",
    " \n",
    "        for m in range(self.boost_size):                   \n",
    "            # Step 2.b\n",
    "            # Compute classification error    \n",
    "            errors = diffs @ w\n",
    "\n",
    "            # indix of the minimum values\n",
    "            min_err_idx = np.argmin(errors)\n",
    "            \n",
    "            # collect classifiers\n",
    "            self.classifiers += [self.cls_boost[min_err_idx]]\n",
    "            \n",
    "            # from lecture: sum of all the weights that were missclassified \n",
    "            # devided by the sum of all weights\n",
    "            w_err = errors[min_err_idx]\n",
    "            w_sum = w.sum()\n",
    "\n",
    "            # The total error is the sum of all the errors in the classified record for sample weights.\n",
    "            E_m = (w_sum - w_err) / w_sum\n",
    "\n",
    "            # Step 2.c\n",
    "            # Compute classifier weight\n",
    "\n",
    "            # Formula for calculating Performance of Stump \n",
    "            alpha_m = -0.5 * np.log((1 - E_m) / E_m)\n",
    "\n",
    "            # Step 2.d\n",
    "            # Recompute sample weights\n",
    "            self.weights += [alpha_m]\n",
    "            \n",
    "            # we must update the sample weight before proceeding for the next model or stage \n",
    "            # because if the same weight is applied, we receive the output from the first model.\n",
    "            \n",
    "            # For incorrectly classified records the formula is:\n",
    "            # New Sample Weight = Sample Weight * e^(Performance) \n",
    "            # And for correctly classified records, we use the same formula with a negative sign with performance, \n",
    "            # so that the weight for correctly classified records will reduce compared to the incorrect classified ones. \n",
    "            # New Sample Weight = Sample Weight * e^- (Performance)\n",
    "            \n",
    "            # self.weights[-1] is the weight for correctly classified records after it was added into array\n",
    "            # returns all indixes of the given row\n",
    "            # non-zero values will be 1\n",
    "            # zero values will be -1\n",
    "            w = w * np.exp(np.where(diffs[min_err_idx], 1, -1) * self.weights[-1])\n",
    "            \n",
    "            diffs = np.delete(diffs, min_err_idx, axis=0)\n",
    "            del self.cls_boost[min_err_idx]\n",
    "            \n",
    "            #print(\"debug: done m = \", m)\n",
    "\n",
    "        # convert into numpy array\n",
    "        print(\"debug: done.\")\n",
    "        self.weights = np.array(self.weights)\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = np.array([cl.predict(X) for cl in self.classifiers])\n",
    "        weighted_preds = np.dot(self.weights, preds)\n",
    "        return np.where(weighted_preds >= 0, 1, -1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak: 13293 of type OneLevelDecisionTree\n"
     ]
    }
   ],
   "source": [
    "boost = classifier_boost(X_train)\n",
    "print (\"Weak: {} of type {}\".format(len(boost), type(boost[0]).__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: done.\n"
     ]
    }
   ],
   "source": [
    "boost_size = 100\n",
    "ada = AdaBoost(boost, boost_size)\n",
    "ada.fit_train(X_train, y_train)\n",
    "ada_predictions = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(labels, predictions):\n",
    "        return np.mean(labels == predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "y_test.shape:  (1151,) ada_predictions.shape:  (1151,)\n",
      "[-1  1 -1 ... -1  1 -1]\n",
      "0.9079061685490878\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape == ada_predictions.shape)\n",
    "print(\"y_test.shape: \", y_test.shape, \"ada_predictions.shape: \",ada_predictions.shape)\n",
    "print(ada_predictions)\n",
    "print(calculate_accuracy(y_test, ada_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[647  56]\n",
      " [ 50 398]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ada_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So I will use my code from previous assignment to build a one\n",
    "# THIS CELL IS COPIED FROM THE PREVIOUS ASSIGNMENT AND CONTAINS IMPLEMENTATION FOR A DECISSION TREE \n",
    "\n",
    "\n",
    "def cross_entropy(p):\n",
    "        if p == 1 or p == 0: \n",
    "            # The entropy is zero if one event is certain\n",
    "            return 0\n",
    "        return - (p * np.log(p) + (1-p) * np.log((1-p)))\n",
    "\n",
    "# Weight of a child node is number of samples in the node/total samples of all child nodes. \n",
    "# Similarly information gain is calculated with gini score. \n",
    "def children_entropy(feature, y):\n",
    "    right = (feature == True).sum()/len(feature)\n",
    "    left = 1 - right\n",
    "    \n",
    "    p = np.sum(y[feature])/len(y[feature]) \n",
    "    q = np.sum(y[np.invert(feature)])/len(y[np.invert(feature)])\n",
    "    \n",
    "    entropy_right = right * cross_entropy(p)\n",
    "    entropy_left = left * cross_entropy(q)\n",
    "    total_entropy = entropy_right + entropy_left\n",
    "    return total_entropy, q, p\n",
    "\n",
    "#====================================\n",
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self, height=7):\n",
    "        self.min_size = 4\n",
    "        self.height = height\n",
    "    \n",
    "    # fit a basic binary tree for 2 classes classificaton \n",
    "    def fit(self, X, y):\n",
    "        self.tree_size = 2**self.height - 1\n",
    "        #print(self.tree_size)\n",
    "        self.tmp_size = 2**(self.height + 1) - 1\n",
    "        self.features = X.shape[1]\n",
    "        self.tree = np.full(self.tmp_size, -1)\n",
    "        self.tree_tmp = np.full(self.tmp_size + 1, -1)\n",
    "        self.split_tree(X, y, 0)\n",
    "    \n",
    "    # binary tree\n",
    "    def left_tree(self, leaf):\n",
    "        return 2 * leaf + 1\n",
    "    \n",
    "    def right_tree(self, leaf):\n",
    "        return 2 * leaf + 2\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            idx = 0\n",
    "            leaf = self.tree[idx]\n",
    "            while self.tree[self.left_tree(idx)] != -1 or self.tree[self.right_tree(idx)] != -1:\n",
    "                #print(\"idx:\", idx)\n",
    "                #print(\"leaf:\", idx)\n",
    "\n",
    "                if leaf >= self.tree_size:\n",
    "                    return\n",
    "                \n",
    "                if x[leaf]:\n",
    "                    idx = self.right_tree(idx)\n",
    "                    #print(\"--------> right\")\n",
    "                else:\n",
    "                    idx = self.left_tree(idx)\n",
    "                    #print(\"left <--------\")\n",
    "                prediction = self.tree_tmp[idx]\n",
    "                leaf = self.tree[idx]\n",
    "            predictions += [prediction]\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def split_data(self, index, value, X):\n",
    "        left, right = list(), list()\n",
    "        for row in X:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "        \n",
    "    \n",
    "    def split_tree(self, X, y, leaf):\n",
    "  \n",
    "        # parent node is a leaf\n",
    "        #print(\"leaf\", leaf)\n",
    "        if leaf >= self.tree_size:\n",
    "            return\n",
    "        \n",
    "        entropies = np.full(self.features, np.inf) \n",
    "        left = np.empty(self.features)\n",
    "        right = np.empty(self.features)\n",
    "        \n",
    "        # for every feature variable\n",
    "        for i, feature in enumerate(X.T):\n",
    "            if np.sum(feature) == 0 or np.sum(np.invert(feature)) == 0:\n",
    "                continue \n",
    "            entropies[i], left[i], right[i] = children_entropy(feature, y)\n",
    "        \n",
    "        min_entropy = np.argmin(entropies)\n",
    "        \n",
    "        right = X[:,min_entropy]\n",
    "        left = np.invert(right)\n",
    "        #print(left)\n",
    "        \n",
    "        #print(\"min_entropy\", min_entropy)\n",
    "        self.tree[leaf] = min_entropy\n",
    "        if min_entropy < len(self.tree_tmp):\n",
    "            if (min_entropy < len(left)) and (min_entropy < len(right)):\n",
    "                self.tree_tmp[self.left_tree(leaf)] = left[min_entropy]\n",
    "                self.tree_tmp[self.right_tree(leaf)] = right[min_entropy]\n",
    "        \n",
    "        if len(y[right]) == 0 or len(y[left]) == 0:\n",
    "            return\n",
    "        # grow tree \n",
    "        if leaf >= self.min_size:\n",
    "            return\n",
    "        self.split_tree(X[left], y[left], self.left_tree(leaf))\n",
    "        self.split_tree(X[right], y[right], self.right_tree(leaf))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_boost_2d(X, y):\n",
    "    boost = []   \n",
    "    # passing by columns\n",
    "    # creates the same number of stumps as the number of features. \n",
    "    for feature in range(X.shape[1]):\n",
    "        # get every unique feature in ordered way\n",
    "        for threshold in sorted(set(X[:, feature])):\n",
    "            tree = DecisionTree(height = 2)\n",
    "            tree.fit(X, y)\n",
    "            boost += [tree]  \n",
    "            '''\n",
    "            boost += [\n",
    "                OneLevelDecisionTree(feature, 1, -1, threshold),\n",
    "                OneLevelDecisionTree(feature, -1, 1, threshold)\n",
    "            ] \n",
    "            '''\n",
    "    return boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = (np.mean(X_train[y_train==1], axis=0) + np.mean(X_train[y_train==-1])) / 2 \n",
    "                  \n",
    "X_train_means = (X_train > means)\n",
    "X_test_means = X_test > means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak: 13193 of type DecisionTree\n"
     ]
    }
   ],
   "source": [
    "boost_tree = classifier_boost_2d(X_train_means, y_train)\n",
    "print (\"Weak: {} of type {}\".format(len(boost), type(boost_tree[0]).__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: done.\n"
     ]
    }
   ],
   "source": [
    "boost_size_tree = 100\n",
    "ada_tree = AdaBoost(boost_tree, boost_size_tree)\n",
    "ada_tree.fit_train(X_train_means, y_train)\n",
    "ada_predictions_tree = ada_tree.predict(X_test_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "y_test.shape:  (1151,) ada_predictions.shape:  (1151,)\n",
      "[-1 -1 -1 ... -1 -1 -1]\n",
      "0.6038227628149435\n",
      "[[647  56]\n",
      " [ 50 398]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape == ada_predictions_tree.shape)\n",
    "print(\"y_test.shape: \", y_test.shape, \"ada_predictions.shape: \",ada_predictions_tree.shape)\n",
    "print(ada_predictions_tree)\n",
    "print(calculate_accuracy(y_test, ada_predictions_tree))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ada_predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** BONUS TASK **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 2 (Bonus).\n",
    "\n",
    "# Viola-Jones Face Detection\n",
    "Implement the Viola-Jones algorithm (without the cascade mechanism) and use it on a LFW-Face-subsetto classify faces.\n",
    "\n",
    "(a) Visualize the top ten face classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolaJonesFaceDetection():\n",
    "    def __init__(self, img):\n",
    "        self.width = img.shape[1]\n",
    "        self.height = img.shape[0]\n",
    "        self.img = img\n",
    "        # integral image to be calculated\n",
    "        self.integral_img = np.zeros_like(img)\n",
    "    \n",
    "    # https://en.wikipedia.org/wiki/Summed-area_table\n",
    "    # The summed-area table can be computed efficiently in a single pass over the image, \n",
    "    # as the value in the summed-area table at (x, y) is:\n",
    "    # I(x,y)= i(x,y) +I(x,y-1) +I(x-1,y)- I(x-1,y-1)\n",
    "    \n",
    "    def calc_integral_image(self):\n",
    "        for y in self.height:\n",
    "            for x in self.width:\n",
    "                self.integral_img[x, y] = self.img[x, y] + self.integral_img[x, y - 1] \n",
    "                + self.integral_img[x - 1, y] - self.integral_img[x - 1, y - 1]\n",
    "        \n",
    "        return self.integral_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 3 (Bonus).\n",
    "# Cascade-Classification\n",
    "\n",
    "Implement a cascade algorithm to classify faces in a picture of your choice \n",
    "(there should be more than a face on your image, e.g. skimage.data.astronaut())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo9GWxKgTN1h"
   },
   "source": [
    "# Mustererkennung/Machine Learning - Assignment 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.347720Z",
     "start_time": "2018-11-29T11:28:47.572823Z"
    },
    "id": "V7XaSv5wTN1i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax8ea49_bkdb"
   },
   "source": [
    "###Load the spam dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.406520Z",
     "start_time": "2018-11-29T11:28:48.349530Z"
    },
    "id": "sT2Hk2k-TN1i"
   },
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('../data/spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "#print(y_test[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the prediction accuracy of the classifier, one needs to split the dataset into a training and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.Decision Trees\n",
    "Implement a decision tree (classification tree to be precise) using Python (incl. Numpy etc.)and use it on the SPAM-Dataset1. \n",
    "Use a metric of your choice as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "# Gini index and entropy are the criteria for calculating information gain. \n",
    "\n",
    "def calculate_gini_index(labels):\n",
    "    # We first need to calculate the proportion of classes in each group.\n",
    "    proportion_ones = np.count_nonzero(labels) / len(labels)\n",
    "\n",
    "    proportion_zeros = np.count_nonzero(labels == 0) / len(labels)\n",
    "\n",
    "    gini = 2 * proportion_zeros * proportion_ones\n",
    "    print(\"Gini Index is {}\".format(gini))\n",
    "    \n",
    "    return gini\n",
    "\n",
    "def cross_entropy(p):\n",
    "        if p == 1 or p == 0: \n",
    "            # The entropy is zero if one event is certain\n",
    "            return 0\n",
    "        return - (p * np.log(p) + (1-p) * np.log((1-p)))\n",
    "\n",
    "# Weight of a child node is number of samples in the node/total samples of all child nodes. \n",
    "# Similarly information gain is calculated with gini score. \n",
    "def children_entropy(feature, y):\n",
    "    right = (feature == True).sum()/len(feature)\n",
    "    left = 1 - right\n",
    "    \n",
    "    p = np.sum(y[feature])/len(y[feature]) \n",
    "    q = np.sum(y[np.invert(feature)])/len(y[np.invert(feature)])\n",
    "    \n",
    "    entropy_right = right * cross_entropy(p)\n",
    "    entropy_left = left * cross_entropy(q)\n",
    "    total_entropy = entropy_right + entropy_left\n",
    "    return total_entropy, q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because an E-Mail can be either SPAM or not SPAM so we will use Gini Index for a 2 classes classifcation tree \n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self, height=7):\n",
    "        self.min_size = 4\n",
    "        self.height = 7\n",
    "    \n",
    "    # fit a basic binary tree for 2 classes classificaton \n",
    "    def fit(self, X, y):\n",
    "        self.tree_size = 2**self.height - 1\n",
    "        #print(self.tree_size)\n",
    "        self.tmp_size = 2**(self.height + 1) - 1\n",
    "        self.features = X.shape[1]\n",
    "        self.tree = np.full(self.tmp_size, -1)\n",
    "        self.tree_tmp = np.full(self.tmp_size + 1, -1)\n",
    "        self.split_tree(X, y, 0)\n",
    "    \n",
    "    # binary tree\n",
    "    def left_tree(self, leaf):\n",
    "        return 2 * leaf + 1\n",
    "    \n",
    "    def right_tree(self, leaf):\n",
    "        return 2 * leaf + 2\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            idx = 0\n",
    "            leaf = self.tree[idx]\n",
    "            while self.tree[self.left_tree(idx)] != -1 or self.tree[self.right_tree(idx)] != -1:\n",
    "                #print(\"idx:\", idx)\n",
    "                #print(\"leaf:\", idx)\n",
    "\n",
    "                if leaf >= self.tree_size:\n",
    "                    return\n",
    "                \n",
    "                if x[leaf]:\n",
    "                    idx = self.right_tree(idx)\n",
    "                    #print(\"--------> right\")\n",
    "                else:\n",
    "                    idx = self.left_tree(idx)\n",
    "                    #print(\"left <--------\")\n",
    "                prediction = self.tree_tmp[idx]\n",
    "                leaf = self.tree[idx]\n",
    "            predictions += [prediction]\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def split_data(self, index, value, X):\n",
    "        left, right = list(), list()\n",
    "        for row in X:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "        \n",
    "    \n",
    "    def split_tree(self, X, y, leaf):\n",
    "  \n",
    "        # parent node is a leaf\n",
    "        #print(\"leaf\", leaf)\n",
    "        if leaf >= self.tree_size:\n",
    "            return\n",
    "        \n",
    "        entropies = np.full(self.features, np.inf) \n",
    "        left = np.empty(self.features)\n",
    "        right = np.empty(self.features)\n",
    "        \n",
    "        # for every feature variable\n",
    "        for i, feature in enumerate(X.T):\n",
    "            if np.sum(feature) == 0 or np.sum(np.invert(feature)) == 0:\n",
    "                continue \n",
    "            entropies[i], left[i], right[i] = children_entropy(feature, y)\n",
    "        \n",
    "        min_entropy = np.argmin(entropies)\n",
    "        \n",
    "        right = X[:,min_entropy]\n",
    "        left = np.invert(right)\n",
    "        #print(left)\n",
    "        \n",
    "        #print(\"min_entropy\", min_entropy)\n",
    "        self.tree[leaf] = min_entropy\n",
    "        if min_entropy < len(self.tree_tmp):\n",
    "            if (min_entropy < len(left)) and (min_entropy < len(right)):\n",
    "                self.tree_tmp[self.left_tree(leaf)] = left[min_entropy]\n",
    "                self.tree_tmp[self.right_tree(leaf)] = right[min_entropy]\n",
    "        \n",
    "        if len(y[right]) == 0 or len(y[left]) == 0:\n",
    "            return\n",
    "        # grow tree \n",
    "        if leaf >= self.min_size:\n",
    "            return\n",
    "        self.split_tree(X[left], y[left], self.left_tree(leaf))\n",
    "        self.split_tree(X[right], y[right], self.right_tree(leaf))\n",
    "            \n",
    "        '''\n",
    "        # calculate split variable\n",
    "        z = (two_rows[0,feature] + two_rows[1,feature]) / 2\n",
    "        print(z)\n",
    "\n",
    "        c1_0 = list((X_tmp[:,feature] <= z) & (y == 0))\n",
    "        c1_1 = list((X_tmp[:,feature] <= z) & (y == 1))\n",
    "        #print(len(c1_1) == len(c1_0))\n",
    "\n",
    "        c2_0 = list((X_tmp[:,feature] > z) & (y == 0))\n",
    "        c2_1 = list((X_tmp[:,feature] > z) & (y == 1))\n",
    "\n",
    "        if len(c1_0) == len(c1_1):\n",
    "            c1 = len(c1_0)\n",
    "        else:\n",
    "            c1 = np.argmax(len(c1_0), len(c1_1))\n",
    "\n",
    "        if len(c2_0) == len(c2_1):\n",
    "            c2 = len(c2_0)\n",
    "        else:\n",
    "            c2 = np.argmax(len(c2_0), len(c2_1))\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini_train = calculate_gini_index(y_train)\n",
    "#gini_test = calculate_gini_index(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# mean of trues and falses \n",
    "\n",
    "means = (np.mean(X_train[y_train==1], axis=0) + np.mean(X_train[y_train==0])) / 2 \n",
    "                  \n",
    "X_train_means = (X_train > means)\n",
    "X_test_means = X_test > means\n",
    "\n",
    "tree = DecisionTree(height = 7)\n",
    "tree.fit(X_train_means, y_train)\n",
    "predictions = tree.predict(X_test_means)\n",
    "\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[204   0]\n",
      " [  0 947]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "estimates = (np.array(predictions) > 0.5)\n",
    "#print(predictions)\n",
    "print(confusion_matrix(predictions, estimates))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your tree to analyze feature importance. \n",
    "Plot the difference between the top 5 features (check spambase.names to check what features those belong to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[274   0]\n",
      " [  0 877]]\n"
     ]
    }
   ],
   "source": [
    "# mean of trues and falses \n",
    "\n",
    "# word_freq_address 1\n",
    "# word_freq_free 15\n",
    "# word_freq_money 23\n",
    "# word_freq_direct 39\n",
    "# word_freq_re 44\n",
    "#idx = [1, 15, 23, 39, 44]\n",
    "X_5 = data[:,idx]\n",
    "\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(X_5, y, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "means_5 = (np.mean(X_train_5[y_train_5==1], axis=0) + np.mean(X_train_5[y_train_5==0])) / 2 \n",
    "                  \n",
    "X_train_means_5 = (X_train_5 > means_5)\n",
    "X_test_means_5 = X_test_5 > means_5\n",
    "\n",
    "tree.fit(X_train_means_5, y_train_5)\n",
    "predictions_5 = tree.predict(X_test_means_5)\n",
    "\n",
    "#print(predictions_5)\n",
    "\n",
    "estimates_5 = (np.array(predictions_5) > 0.5)\n",
    "#print(predictions)\n",
    "print(confusion_matrix(predictions_5, estimates_5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used: \n",
    "word_freq_address 1 \n",
    "word_freq_free 15\n",
    "word_freq_money 23\n",
    "word_freq_direct 39\n",
    "word_freq_re 44\n",
    "\n",
    "**There are more true positive results than on the whole set of features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Random Forest and use it on the SPAM-Dataset.\n",
    "\n",
    "a) Print a confusion matrix (you can use package implementations here).\n",
    "\n",
    "b) What is a good number of trees in the forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \n",
    "    def __init__(self, height=7, n_trees = 100):\n",
    "        self.n_trees = n_trees\n",
    "        self.height = height\n",
    "        self.trees = [DecisionTree(height = height)\n",
    "                      for _ in range(n_trees)]\n",
    "    \n",
    "    def fit(self, X, y, n_samples = 500):        \n",
    "        for tree in self.trees:\n",
    "            # pick randomly the datapoints\n",
    "            #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, shuffle=True, stratify=y)\n",
    "            random_samples = np.random.randint(0, high=len(X), size=n_samples)\n",
    "            X_train = X[random_samples]\n",
    "            y_train = y[random_samples]\n",
    "            random_features = np.random.randint(0, high=len(X.T), size=self.height*2)\n",
    "            X_train = X_train[:,random_features]          \n",
    "            \n",
    "            means = (np.mean(X_train[y_train==1], axis=0) + np.mean(X_train[y_train==0])) / 2       \n",
    "            X_train_means = (X_train > means)\n",
    "            tree.fit(X_train_means, y_train)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        # Version 1\n",
    "        #preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        #preds = np.mean(preds, axis=0)\n",
    "        #return preds\n",
    "        \n",
    "        # Version 2 \n",
    "        forest_predictions = np.array(self.trees[0].predict(X))\n",
    "        #print(forest_predictions.shape)\n",
    "\n",
    "        # make it as row vector by inserting an axis along first dimension\n",
    "        forest_predictions = forest_predictions[:, np.newaxis]\n",
    "        #print(forest_predictions.shape)\n",
    "        \n",
    "        for i in range(1, self.n_trees):\n",
    "            pred = np.array(self.trees[i].predict(X))\n",
    "            # When axis is specified, values must have the correct shape.\n",
    "            forest_predictions = np.append(forest_predictions, pred[:, np.newaxis], axis=1)\n",
    "        \n",
    "\n",
    "        forest_average = np.array(np.mean(forest_predictions, axis=0))\n",
    "        return forest_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "trees:  25\n",
      "[[ 2  0]\n",
      " [ 0 23]]\n",
      "-----\n",
      "-----\n",
      "trees:  50\n",
      "[[ 3  0]\n",
      " [ 0 47]]\n",
      "-----\n",
      "-----\n",
      "trees:  75\n",
      "[[12  0]\n",
      " [ 0 63]]\n",
      "-----\n",
      "-----\n",
      "trees:  100\n",
      "[[18  0]\n",
      " [ 0 82]]\n",
      "-----\n",
      "-----\n",
      "trees:  125\n",
      "[[ 11   0]\n",
      " [  0 114]]\n",
      "-----\n",
      "-----\n",
      "trees:  150\n",
      "[[ 22   0]\n",
      " [  0 128]]\n",
      "-----\n",
      "-----\n",
      "trees:  175\n",
      "[[ 16   0]\n",
      " [  0 159]]\n",
      "-----\n",
      "-----\n",
      "trees:  200\n",
      "[[ 25   0]\n",
      " [  0 175]]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for t in range(25, 201, 25):\n",
    "    random_forest = RandomForest(height=7, n_trees=t)\n",
    "    random_forest.fit(X, y, n_samples = 1000)\n",
    "    predictions_rf = random_forest.predict(X_test_means)\n",
    "    #print(predictions_rf)\n",
    "\n",
    "    estimates_rf = (np.array(predictions_rf) > 0.5)\n",
    "    print(\"-----\")\n",
    "    print(\"trees: \", t)\n",
    "    print(confusion_matrix(predictions_rf.round(), estimates_rf.round()))\n",
    "    print(\"-----\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ensembles.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
